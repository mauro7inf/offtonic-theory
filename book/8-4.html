<?php
error_reporting(E_ALL);

require($_SERVER['DOCUMENT_ROOT'] . '/theory/book.php');
$label = 'composition layers';

createPageHeader($label);

?>

<p class="text">A piece of music can be roughly divided into abstract layers of musical meaning.  I guess this is more about music in general than about composition specifically, but it's as good a place as any to talk about it, right?  But we'll talk about how to approach these layers.  The idea is that some aspects of the music — the beat, the melody, the lyrics — are much more powerful than other aspects — the dynamics, the timbres, etc. — so when your music has the powerful aspects, you pay attention to those, but when it doesn't, you pay attention to the less powerful ones.  I think of it like a Taylor series.</p>
<h4 class="question">Oh, great, <em>more</em> math.</h4>
<p class="text">I was taking freshman physics when I first thought of this, all right?  I actually remember it pretty clearly.  Freshman year of college, a friend of mine was in an orchestra that was playing a really cool thing: a piece for 12 strings, 6 of which were tuned normally to A 440 and the other 6 of which were tuned a quarter step sharp, to A 453.  At the time I had a budding interest in microtonality in general, so I was really excited for it; the piece was Gyorgy Ligeti's <a href="https://www.youtube.com/watch?v=FAuAFEFEFSo">Ramifications</a>.  I was on the edge of my seat the entire time while the people I sat next to were bored by the piece.  To be honest, I don't even remember the piece itself that well, but it wasn't at all what I had expected.  There was no melody or harmony.  There was just... <em>sound</em>.  But it was very <em>interesting</em> sound.  It brought me to the realization that the various aspects of music are sort of like a Taylor series, where the leading terms are usually much bigger than the latter terms, but sometimes the leading terms are 0 so the next biggest term is the most important, and so on.  More concretely, melody is more salient than harmony, so if a piece has melody and harmony, you'll pay attention to the melody more.  If a piece has harmony but <em>not</em> melody, then you'll pay attention to the harmony instead.</p>
<p class="text">That's why it makes sense to talk about these aspects as layers, because if the top layer isn't there, the next layer is the most visible.  But these are all somewhat transparent layers; you can hear all of them.  It's just that you pay the most attention to the outer ones.</p>
<p class="text">We can roughly summarize the layers as follows:</p>
<dl class="definitions">
	<dt>Top layers:</dt>
	<dd>beat, tempo, melody, words, sound</dd>
	<dt>Middle layers:</dt>
	<dd>harmony, mode, rhythm</dd>
	<dt>Bottom layers:</dt>
	<dd>dynamics, timbre, pitch, orchestration, sound</dd>
</dl>
<p class="text">We'll talk about each of these in this section.</p>

<? createTableOfContents($label) ?>

<h3 class="subsection-title" id="top-layers">8.4.1 The Top Layers</h3>

<p class="text">The topmost layers of the musical experience are the ones that are hard-wired into our brains: beat, melody, words, and (some aspects of) sound.  All of these are features which we as humans have evolved the ability to perceive, and they essentially explain why music is so compelling to us — why music exists in the first place.  We've taken our primal appreciation for these aspects and created something that satisfied them: music.</p>
<p class="text">We'll start with the beat.  <a href="https://www.frontiersin.org/articles/10.3389/fnhum.2017.00303/full">You can read more about it at Frontiers</a> if you're interested; rhythm cognition is an active area of scientific research, and I could guess stuff but there's, you know, real science.</p>
<h4 class="question">Heh.  You trust <em>scientists</em>?  Those ivory-tower academics in their ivory towers, scamming the government for research grants, creating worldwide crises just to publish more papers.  Bah, scientists!</h4>
<p class="text">...</p>
<p class="text">...</p>
<p class="text">...</p>
<h4 class="question">Dude, I'm kidding!  I love science!</h4>
<p class="text">Whew.  I thought you'd gone off in the head!</p>
<p class="text">Now, musically, the beat is the constant pulse that we <em>feel</em> in a piece of music.  You can have a pulse that you don't actually feel.  That doesn't count; you have to be able to feel it.  Usually, stronger beats are regular — the same thing over and over — in a fairly tight and natural pattern.  We've seen this before in <? exampleLink('basic 4/4 beat'); ?>:</p>

<? createExample('basic 4/4 beat', true); ?>

<p class="text">You could ostensibly play this example on repeat for a long time and not get bored.  Why?  Because it triggers your rhythm perception.  This rhythm in particular is very predictable and natural, and it has a low frequency component (the bass drum) that helps you feel it in your body.</p>
<p class="text">A repetitive pattern like this is also hypnotizing.  Musicologist <a href="http://www.josephjordania.com/battle-trance-and-collective-identity.html">Joseph Jordania</a> calls this hypnotic state "battle trance", and this is part of what makes synchronized things so damn satisfying to do.  Taking part in making music that's really well together, like a tribal war chant or, more applicably in the modern world, a sports chant, <em>feels</em> great; it <em>feels</em> energizing.  The hypnotic effect of the beat is basically the brain getting used to it, but instead of tuning the beat out, it tunes <em>you</em> in.  It makes you start moving to the beat.  It makes you want to tap your foot, or clap your hands, or snap your fingers, or gyrate your hips, or march, or just generally dance.  When you're "tuned into" the beat, we call that beat a <em>groove</em>; I don't know why it's called that, but if you think of the groove on a record, being "in" the beat is like the needle being in that groove; it's clicked in, so it's easy for it to stay inside.</p>
<p class="text">The beat is primal.  Any sort of dance music will have a heavy focus on the beat, no matter what genre or time period or culture.  On the other hand, "higher" art music that deliberately <em>avoids</em> appealing to humankind's "base" instincts will avoid a strong beat.</p>
<h4 class="question">That seems somewhat pretentious, no?</h4>
<p class="text">Well, not really, specifically because of the layer effect we're talking about.  A strong beat is a cheap thrill, but it takes away from the depth of the rest of the music.  If you want your music to be complex and interesting, you may not want to hide that complexity underneath a big ol' layer of beats.</p>
<p class="text">Some people write great music without a strong beat.  Liszt's <a href="https://www.youtube.com/watch?v=goeOUTRy2es">Hungarian Rhapsody No. 2</a> (it was originally written for piano, but it was orchestrated by Liszt and Doppler later on) does have a beat at times, but it's not as strong as in modern popular music.  The fast section (the <em>friska</em>) begins at 6:03 after a fairly long slow section (the <em>lassan</em>).  A shorter version of the lassan is featured in a whole bunch of cartoons from the 1930's; this is probably Liszt's best-known works.  The Can-Can-like beat, and the fast tempo too, make this piece extremely exciting and a crowd favorite.  But that wasn't enough for <a href="https://www.youtube.com/watch?v=iUPOY1earq8">Paul Mauriat</a>, oh no!  My parents had the Mauriat CD when I was growing up, and that's how I knew the piece.  I don't mind the synthesizers or the fact that the piece is much shorter, but he had to go and add <em>drums</em> to everything.  What's the deal?  I think people are used to a certain level of excitement from their music, and the only way classical music appeals to these listeners is if drums are added.  It's strange, because the original is very exciting, and if anything, the one with the drums is <em>less</em> exciting.</p>
<p class="text">The beat in classical music in general is almost always far weaker than the beat in popular music in general.  You could argue that the beat is what makes popular music <em>popular</em>.  Listening to popular music is a primal, visceral experience in large part because of the beat (and the sound, but we'll talk about that in a bit).  And that primal, visceral experience contrasts with the experience of classical music, even when that music does have a beat, because the classical beat doesn't get defined as sharply, usually.  We'll talk about the instrumentation in a later chapter, but in a pop recording or concert, the bass eq is turned <em>way</em> up, making the kick drum something you feel with your gut and making the amplified electric bass something you feel with your entire apartment (if someone's listening to music loudly in another apartment, the bass is all you'll hear).  The more you turn up the bass, the more awesome it sounds.  (Or at least that's what you tell yourself.  Eventually, you'll hear bass that's turned up <em>too</em> loud, and you'll realize that there is such a thing as too much bass.)</p>
<h4 class="question">Do you have a clever analogy for that?</h4>
<p class="text">Why, yes, I do!</p>
<h4 class="question">...Are you going to tell us the analogy?</h4>
<p class="text">Nope!</p>
<p class="text">Closely related to the beat is the tempo.  You could even think of the tempo as part of the beat — it's the speed of the beat, after all.  We humans in fact use the beat to discern the tempo.  In that Liszt example, the tempo actually gets slower and faster during the fast section, creating and releasing tension.  Usually the faster stuff is more exciting.  The Paul Mauriat version, with the drums, keeps the same tempo throughout (during the fast section), which I think makes it less exciting.</p>
<p class="text">I'm going to group beat and tempo together in the <em>rhythmic feel</em> of a piece.  This is separate from the rhythm, which we'll talk about a bit later.  A slow tempo is automatically relaxing, especially close to 60 or 70 BPM.  Slower than that, and it becomes more difficult to perceive a beat as a single entity.  Similarly, a fast tempo is automatically exciting, up to about 200 BPM.  Faster than that and it's kind of too fast to hear the beats as individual pulses.  This relaxation or excitement is biological; it's not just some cultural connotation.  We humans perceive tempo as a way of setting our own working speed.  If you're doing aerobic exercise, you may even find that you can go longer if you're listening to upbeat music, and fast music can pump you up when you need a little adrenaline for some task.  On the other hand, slow music can help you fall asleep — at least provided it's boring enough that you don't pay attention to it.  Unfortunately, I can't really do that myself; if I put music on, I will always pay attention.</p>
<p class="text">The rhythmic feel can also be subdivided in different ways; different parts of the beat can be emphasized, etc.  We talked about some of these things back in <? sectionLinkShort('rhythm'); ?>; the time signature can be a fairly good representation of rhythmic feel, but there are many ways to interpret a time signature; a Mozart string quartet in 4/4 is probably going to have a different rhythmic feel from a heavy EDM piece in the same time signature.  Remember also that time signatures only exist in sheet music; a recording does not have a time signature.  There may be a time signature that would be most natural for the recording, but you can technically fit any music into any time signature if you set the tempo well enough, and use the right tuplets enough, and hate your players enough.</p>
<p class="text">After the beat/tempo/rhythmic feel comes the melody layer.  Melody, again, is one of those things that humans have evolved to perceive.  I'm not sure why that is, because I'm not an evolutionary neurobiologist.  You can <a href="http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780198525202.001.0001/acprof-9780198525202-chapter-21">do your own research</a> if you're interested.  (Maybe someday I'll read those books and papers and update this section, but until then, I'm going to stick to the stuff I know.)  What I know is that melody is absolutely integral to our perception of music.  Various other animals perceive melody as well; songbirds need to be able to distinguish between calls from different species, between warning calls and food calls, etc.  For most of us (who don't have perfect pitch), perceiving a melody means perceiving the intervals that make it up in some way (specifically, the intervals with some static tonal center, not between adjacent notes, because it's too difficult to keep switching reference points).  We can generally tell whether two melodies are the same even if they're in different keys.</p>
<h4 class="question">What if you're tone-deaf?</h4>
<p class="text">Tone deafness (<a href="https://en.wikipedia.org/wiki/Amusia">amusia</a>) basically means that you can't understand melodies, because you can't process pitch, to some extent (I don't know if you can tell between a tuba and a piccolo; I'd assume you can, but if they're playing the same note octaves apart you wouldn't know).  Melody is higher-level than pitch and is independent of a tonal center; the same melody is recognized as such whether it's played high or low.  Pitch, on the other hand, is a lower layer than melody; we'll get to it.</p>
<p class="text">On the other side of tone deafness is <a href="https://en.wikipedia.org/wiki/Absolute_pitch">perfect pitch</a>.  That can <em>also</em> cause problems with melody, because a person with perfect pitch may have some difficulty hearing a melody played in one key as being the same as the melody played in a different key.  Perfect pitch is a superpower in a sense but a drawback in the case of hearing melodies.</p>
<p class="text">The melody is the singable part of a song.  Pieces can have a single obvious melody — like pretty much all folk songs, most non-rap pop songs, a lot of musical theater songs, many jazz standards, and... some classical music, I guess — or they can have fragmentary melodies, possibly many of them at once, or alternating or something.  This is incredibly common in classical music, but less so in other genres.  An improv solo on a horn (in jazz, <em>horn</em> refers to trumpets, trombones, or saxes, not actual French horns) is essentially a melody too, but it's usually not a very... melodic melody.  Musical theater frequently has multiple people singing different melodies together.  A fugue generally has snatches of melody happening all the time, but not really a single complete melody.</p>
<p class="text">At least in Western culture, humans tend to really like melody.  That's why there's a strong reaction against rap by the older generation, who doesn't think of rap as music.  If there's no melody, where's the music?  Rap is just people talking!  Those people would have some trouble listening to melody-less minimalist pieces too.  Melody makes music accessible, but on a level higher than the beat.  If music is a book, the beat is the pictures and the melody is the story.  Kids love a picture book with a good, easy-to-follow story.  As they grow up, they may want to branch into other genres, or read books with multiple sub-plots and no pictures, or even explore the world of art books with no text.</p>
<p class="text">Melodies can be catchy, especially if you repeat them often enough.  If the melody is singable enough, meaning that it easily fits a regular person's singing range, without difficult intervals and awkward jumps, and it's a complete melody (rather than just a fragment) that gets repeated often enough, and it's exciting enough due to rhythmic feel, it <em>will</em> get stuck in people's heads.  That's called an <a href="https://en.wikipedia.org/wiki/Earworm">earworm</a>, also called <em>involuntary musical imagery</em> (INMI), and it's another neurological effect of melody.  Melody is important, guys!  Getting melodies stuck in people's heads drives music revenue, though, so composers and songwriters do this shit <em>on purpose</em>.  If the music is catchy enough, people will remember it and it will sell.  (Melody isn't the only aspect that can be catchy, but it's one of the primary drivers of the phenomenon.)</p>
<p class="text">Lots of music is written without melody, or with melody only occasionally.  Rap, for example, generally has no melody other than perhaps a chorus, and even then, the chorus may be rapped rather than sung.  The background instrumentals may have something more melodic, but rap generally works over a fairly short loop, so the melodic value is usually very small.  Modern atonal music often has no melody, and when it does, it deliberately avoids a pitch center, making it difficult to remember and sing (we generally remember melodies by scale degree).  Religious chants may not have a real melody either; in Judaism at least, the chants are generally sequences of motifs.  That makes them somewhat similar to symphonies, which often also use motifs rather than complete melodies, but classical music usually has more of a beat than these religious chants.</p>
<p class="text">You would think that music for unpitched percussion can't have a melody.  You'd be... right, mostly.  But, interestingly enough, unpitched percussion <em>can</em> have melodic <em>contour</em>.  It's called "unpitched percussion", but it's not <em>really</em> unpitched.  A big snare drum is lower than a little snare drum, for example, and if you have several different snare drums of different sizes, you can play little melodies there, even though there's no definite pitch.  I've mentioned the all-12-notes final chord of Vincent Persichetti's <a href="https://www.youtube.com/watch?v=xjocQ4SQuVQ">Symphony No. 6</a>, but that piece is also famous for its use of unpitched percussion playing motifs, including three differently-sized snare drums.  Listen for the toms at the beginning of the fast section in the first movement (1:42); the xylophone is playing a melody, but the toms are playing the melody too!  I've played percussion on this piece before; it's a beast (the clarinet part is much easier; I've played that too).  You can sometimes find melody in unlikely places!</p>
<p class="text">Next layer: words.  I hopefully don't need to explain <a href="https://en.wikipedia.org/wiki/Neurolinguistics">neurolinguistics</a> to you.</p>
<h4 class="question">What's that?</h4>
<p class="text">Damn it.  I don't know; I didn't read the article!  Actually, it says "Not to be confused with <a href="https://en.wikipedia.org/wiki/Cognitive_linguistics">Cognitive linguistics</a>", which is funny because I had totally confused the two.  But the basic point is that humans evolved the ability to use language as part of the human brain's basic functionality.  When we hear words, we don't just hear a sequence of sounds (though you should try to listen to speech as sound sometime to hear what it's like — it's easy if you're listening to someone speak a language you don't understand).  We hear syllables and words and, hopefully, meanings.  With music, words carry meaning in a way that all other musical aspects don't; people like to say that music is a language, but really, it's not.  Music is music; <em>language</em> is language!  Language can communicate thoughts and ideas.  Music can't, unless it too uses language.  It's that simple!</p>
<p class="text">Since we humans are so primed to hear and understand words (sometimes <a href="https://en.wikipedia.org/wiki/Pareidolia">even where there aren't any</a>), if there are audible and understandable words in the music, we <em>will</em> pay attention to them, and we will process their meaning.  Some singing styles, like operatic singing, make actually understanding the words difficult, especially when the words are sung in a language you don't actually understand.  Other singing styles, like folk songs and most pop, are quite understandable.  Sometimes the singing is just too fast or too slow to be easily heard unless you're paying attention.  Eminem's <a href="https://www.youtube.com/watch?v=XbGs_qK2PQA">Rap God</a> is particularly famous for its sheer volume of words; start listening to the beat for a bit (and stop focusing on the words) and you'll quickly lose your place.</p>
<p class="text">Assuming that the lyrics are clear and understandable, the reason they're so important is that they take over the song in the mind of the listener.  When you're listening to the lyrics, you're not really paying so much attention to everything else unless you zone out and stop listening to the words on purpose.  If the lyrics are simple enough, or they're repeated, the listener has more of a chance to explore the rest of the song's layers.</p>
<p class="text">Lyrics are extremely important in most formats in which they're present.  Popular music would not work at all without words.  Rap obviously needs the words, but that goes without saying.  Folk songs need the words.  Showtunes need the words.  There are genres where the words are less important, though.  In classical music, composers tend to bring other compositional elements into the foreground.  (That's a huge generalization, but it does happen.)  Here's an example, Bach's cantata <a href="https://www.youtube.com/watch?v=rZjVP6y71yg">Weinen, Klagen, Sorgen, Zagen</a> (Google translates this to "Cry, Complain, Worry, Quarrel"), beginning where the words start at about 2:45, after the intro (don't look at the clefs or you will cry), and the movement ends at about 5:09.  The words aren't exactly enrapturing, since they're not telling a story.  You hear them once; now you know what they are, and you can pay attention to the beautiful harmony and counterpoint.  The first minute plus of the movement is just these four words, so the words are really the vehicle for the music rather than the other way around.  More words do come in eventually, but it's the same deal.  In this sort of polyphonic texture, with people essentially singing over each other, it's difficult to understand words unless you already know what they are.</p>
<p class="text">In the opposite extreme, there are songs where the music is a vehicle for the words.  Songs that tell stories are a big example; here's Weird Al's <a href="https://www.youtube.com/watch?v=YY6I4MwrX7k">Jackson Park Express</a>, which features the wonderful lyric "I'd like to remove all your skin, and wear your skin over my own skin, but not in a creepy way."  The melody is kind of generic and meandering, but you're not really supposed to be listening for the melody.  Another one, Jason Robert Brown's <a href="https://www.youtube.com/watch?v=paMvQ4rziqc">Schmuel's Song</a> from his play The Last Five Years (which apparently was made into a poorly-reviewed movie with Anna Kendrick and Jeremy Jordan).  This one's fun and has some odd time signatures and a catchy melody (and also, with some changes at the end, it could be a High Holiday sermon — wouldn't be the first time I've heard one of those reference a movie with Anna Kendrick).  Last one, Stephen Schwartz's <a href="https://www.youtube.com/watch?v=BiCW8_m7h_4">Meadowlark</a>, from The Baker's Wife, here featuring Patti LuPone (I don't like the Sarah Brightman recording, personally, but I'm not sure I like this one either).  The story bit starts at about 1:20, and it's a stunningly beautiful song with stunningly beautiful harmonies.  It's funny, I just noticed that Schmuel's Song and Meadowlark have very similar morals; both songs tell their stories as allegories about opportunity and taking the chance to be happy, but with different approaches.</p>
<p class="text">Many sung works use words from existing sources, generally poetry but, in the case of religious chants, they can also be prose.  The music may match the structure of the text or it may not.  For example, in the 19th century there was a German tradition of setting songs for voice and piano called <em>lieder</em> (a <em>lied</em> is just a song in German), with Franz Schubert being the most famous composer in this genre.  Goethe's poem <a href="https://en.wikipedia.org/wiki/Erlk%C3%B6nig_(Goethe)">Erlkönig</a> was famously set by both <a href="https://www.youtube.com/watch?v=4evXatdMuEY">Carl Loewe</a> and <a href="https://www.youtube.com/watch?v=JS91p-vmSf0">Schubert</a>, each without knowledge of the other.  These two specific settings are frequently paired to show the different approaches to setting poetry to music.  Loewe opts for a kind of verse structure, where different verses have similar melodies that outline the meter and rhyming scheme of the poem, while Schubert has a much freer interpretation and his work is generally regarded as the more musically significant setting, whatever that means.</p>
<p class="text">Song lyrics can come in many shapes.  We'll talk about that in a later chapter.  Before we do that, though, I'd like to make the connection between songs and poetry: poems are songs <em>without music</em>.  The words have to be carried on their own meaning and sound.  Songs also have music, and that means that the words can serve the music instead of the other way around if the composer wants.  This means that good song lyrics don't necessarily make good poetry, and vice-versa.  A strong understanding of poetics in general is necessary to write good lyrics, but they require different approaches from tuneless poetry.  You can think of rap music as sort of in between these two ways of dealing with words, closer to the music side, while spoken word is also in between but closer to the poetry side.</p>
<p class="text">Finally, music can also have narration, which is when someone is talking, not singing, while music is playing (or possibly during rests).  The effect may well be that people pay attention to the narration rather than the music.  This could be a story, like Prokofiev's <a href="https://www.youtube.com/watch?v=9ueGfjBKbiE">Peter and the Wolf</a>, here narrated by Itzhak Perlman, or more generic talking, like Copland's <a href="https://www.youtube.com/watch?v=8P-Ep-GsTVg">Lincoln Portrait</a>, here played in an abbreviated form by the Boston Pops and narrated by all of the living former presents in 2002 (Ford, Carter, Reagan, Bush, Clinton, though Nancy Reagan read her husband's part).  I suggest finding a complete recording of Copland's piece, since this one skips a lot of it and goes pretty much straight into the narration, but the narration by the former presidents is just too awesome to pass up.  It's worth noting that Aaron Copland was politically as left as left gets, and the American spirit he was trying to foster was strongly progressive, focusing on economic justice and dignity for all, in response to the European fascism he was fighting against.  He stopped writing his iconic Americana when he became disillusioned with what America became under McCarthyism; you can <a href="https://www.chicagomaroon.com/2004/3/5/aaron-copland-the-man-his-music-and-mccarthyism/">read about it here</a> (or find one of the many other articles written about Copland's political life) if you want.</p>
<p class="text">Closely related to the words are... whatever else happens during the performance — dancing, multimedia, etc.  I don't really see those things as part of the music.  The story in an opera, the actors, the costumes, all that stuff, I don't think that's music (we can go back to our definition in the <? sectionLinkShort('intro'); ?>).  If there's a music video, or there's some sort of animation or light show (like in Scriabin's <a href="https://www.youtube.com/watch?v=V3B7uQ5K0IU">Prometheus</a>), or a marching show with flags and dancers, or a ballet, any of that stuff, that's not part of the music.  Similarly, <em>incidental music</em> is music that plays during a play or film; the play or film, in this case, is not part of the music.  All that stuff can carry meaning and can generally pull attention away from the music itself.  Many films and shows have quite boring incidental music, for example, for the opposite reason: they want the music to provide a backdrop, but they don't want it to get in the way of the film or show itself.  Old cartoons, which had very little speaking so most of the auditory component was the incidental music, went a different way: called <a href="https://en.wikipedia.org/wiki/Mickey_Mousing">Mickey Mousing</a> (named after... Walt Disney's Mickey Mouse, obviously), the music is synchronized with the events on the screen, so when a character steps on a rake, the music might have a cymbal crash, for example, and slow footsteps could be timed with <a href="https://en.wikipedia.org/wiki/Mysterioso_Pizzicato">soft pizzicato strings</a>.  Note that the music was usually written before the animation so that the animation frames could be precisely aligned with the musical hits.  In this case, the action is still not part of the music, but rather, the music is part of the action.</p>
<p class="text">Words have been an intrinsic part of music probably since the very beginning of music.  I've only mentioned a few of the ways in which composers and songwriters incorporate words into their music.  We'll talk more about words later, when we look at poetic meter.  Writing lyrics is a totally different skill from writing the other aspects of music, and not all great composers are also great lyricists.  Some are, but many are not.  I consider myself pretty good with the music stuff, but you do <em>not</em> want to hear my attempt at writing a pop-style song back in high school.</p>
<h4 class="question">Hey!  Sure I do; play it!</h4>
<p class="text">I think I came up with a good, if 80's-ish, melody and harmony, but man, those lyrics...  The funny thing is that I remember that first verse I wrote better than I remember the girl whose breakup with me inspired the song.</p>
<h4 class="question">Oh, it's one of <em>those</em>.  Yeah, I don't have any interest in hearing it anymore.</h4>
<p class="text">See?  I told you!  High school breakups, man.  They suck at the time, and then later on in life they come back to suck again when you remember them and cringe at your behavior back then.  Still, I think I had something good with that song.  It just sounded like every pop song from the 80's ever, despite this taking place in...  2001?  Sounds about right.</p>
<p class="text">OK, jumping <em>out</em> of that rabbit hole...  The last layer in this topmost set of layers is the sound.  As a musical layer, the sound is poorly defined, so I can't easily link it to some aspect of neurophysiology.  By the sound I mean the overall experience of hearing the music without the musical <em>content</em>.  As an example, listen to the famous <a href="https://www.youtube.com/watch?v=uYMpMcmpfkI">Deep Note</a>, the THX logo theme (THX actually released <a href=https://en.wikipedia.org/wiki/Deep_Note">sheet music</a> for it, too).  The sound is majestic and <em>striking</em>, isn't it?  They play it at the start of a movie in the movie theater, and it gets you excited, in part because the theater's sound system makes it sound impressive.  A full sound has stuff happening at both low and high registers and everywhere in the middle; a sparser sound may not.  James Moorer, composer and programmer of Deep Note, may or may not have been inspired by the opening (specifically the first two bars, before any melody begins) to <a href="https://www.youtube.com/watch?v=ypClfhEwwCw">Mahler 1</a>, which I strongly recommend that you listen to with headphones, because computer speakers can't generally play the sound very well unless you turn up the volume <em>way</em> up.  In these first two bars, the strings play very low and very high A's, all at <em><strong>pp</strong></em> or <em><strong>ppp</strong></em>.  The "Flag." written next to the notes indicates <em>flageolet</em>, which simply means playing harmonics.  We'll talk about string physics in a later chapter —</p>
<h4 class="question">We will?  What's this, a physics class on waves?  Will there be differential equations too?  Man.  This is music theory, not university-level physics.</h4>
<p class="text">Yes, we will.  Turns out it's actually really important to understand that stuff when you want to understand sound in general.  Now, to get back to what I was saying, in the opening two bars of Mahler's Symphony Nº1 "Titan", the strings play flageolet, which means harmonics.  The player fingers a high A and <em>lightly</em> touches a magic spot on the string (the high D, a fourth up); this causes the string to vibrate with that point as a node (again, we'll talk about the physics later), resulting in a much higher sound, two octaves up in this case, with a kind of ethereal quality.  This is what Mahler does here.  Combining this very high A with the low A of the basses (remember that the bass plays an octave lower than written, so the written A2 is actually an A1, for a 6-octave spread between the low A of the basses and the high A of the violins) at these low dynamics creates a sound that evokes the expansiveness of nature, at least according to a professor I had once.  By the way, the symphony as a whole is pretty great too, if you want to listen to it.</p>
<p class="text">In a totally different vein, here's Reel Big Fish's <a href="https://www.youtube.com/watch?v=AEKbFMvkLIc">Sell Out</a> (starts about 30 seconds in).  The song starts with a blast of horns, drums, and guitar/bass, suddenly and excitingly.  The overall sound of the piece is defined by these instruments, the voices of the singers, how they sing in harmony, their diction and pronunciation, etc.  Oh, and the mixing and mastering.  You can bet that the band didn't sound that good live.  That's what we talked about in <? sectionLinkShort('composition product'); ?>.  Fans tend to really care about the sound, which is why big subwoofers are so popular.  Loud music is exciting because the loudness is part of the sound.  On the other hand, a synthesized recording just doesn't sound as nice.</p>
<p class="text">Chiptunes have found a niche these days, but many people find the sound grating.  Listen for yourself; here's the theme from <a href="https://www.youtube.com/watch?v=uyMKWJ5e1kg">Legend of Zelda</a> on the NES.  To people who grew up on the NES, this sounds like nostalgia, but to people who didn't, this sound is awful.  I didn't personally grow up on the NES — I had an Atari as a kid, and then an SNES, but we skipped the NES — but as a lover of videogame music I've had to come around to it.</p>
<p class="text">Here's Darius Milhaud conducting his own <a href="https://www.youtube.com/watch?v=JLgrCsfXjSQ">Suite Française</a>, a staple of the wind orchestra repertoire, which he later adapted for symphonic orchestra.  The recording is not so high-fidelity, though, so the sound got... warmer, somehow, like the sound of a vinyl record.  Of course Milhaud didn't intend for his piece to sound like that; he wrote it for actual instruments.  To me this is also a nostalgic sound, like the soundtrack to an old movie, the kind that you want to play on a rainy afternoon.  You can affect this sound with effects during mixing, but in this recording it was not done this way on purpose.</p>
<p class="text">There are a lot more examples of noteworthy sound in pieces of music, but every piece has a sound; we can't possibly go through all of them!  One thing I will say, though, is that <em>bands</em> tend to have pretty consistent sound across their music.  I showed you Reel Big Fish's Sell Out, but their other stuff sounds about the same.  The fans fall in love with <em>that</em> sound, so they can't really change it too much.  Orchestral composers don't generally have that kind of limitation.  The sound occupies a higher layer to listeners of popular music than to listeners of classical music, which is why pop bands end up defined by their sound; for many bands, you can identify them from just a few seconds of their music, even if you don't know the song.  The effect is that this first impression of the sound can quickly cement a listener's opinion of an unfamiliar piece of music.  It's like judging a book by its first sentence.</p>
<h4 class="question">"Call me Ishmael."  Story about some dude with the Biblical name of an outcast, probably full of bullshit symbolism.  No thanks!</h4>
<p class="text">I don't think that's what Moby-Dick is about (other than the bullshit symbolism), but if it saves you from reading it, I'd consider that a win.</p>
<h4 class="question">That was sarcasm!  Moby-Dick is a stellar piece of writing; it's the Great American Novel!</h4>
<p class="text">I'm going to choose to ignore what you just said.</p>
<p class="text">One important feature of the sound is that it colors your perception of the rest of the music, but once you get past your impression of the sound, you actually pay attention to everything else.  This is true for the other layers as well.  The more one particular layer doesn't change, the more your ears adjust to hear beyond it.  These top layers all scream their presence thanks to the way the human brain perceives sound in general, but the human brain is also great at deemphasizing repetitive signals, meaning that the novelty fades.  The issue is that the other layers are still less important, less definitional.  When you actually eliminate the top layers, the layers beneath don't just become <em>interesting</em>; they become <em>the piece</em>.</p>
<p class="text">So let's go a bit deeper:</p>

<h3 class="subsection-title" id="middle-layers">8.4.2 The Middle Layers</h3>

<p class="text">The next set of layers correspond to the emergent properties of music that have become culturally important: harmony, mode, and rhythm.  The harmonies, modes, and rhythms that we find agreeable are not determined by biology or mathematics but rather by our upbringing and our musical culture.  This should be clear from <? sectionLinkShort('modes'); ?>, where we got to experience music with no harmony and with modes that sound alien to Western ears, and that music is nevertheless enjoyed by non-Western cultures.  Music is very much a product of its time and place, and the details of mode and rhythm are the biggest variables.  Harmony generally doesn't even exist in other times and places; it's primarily a Western innovation.  Let's start with that.</p>
<p class="text">The harmony layer is generally subordinate to the melody layer.  Harmony is there to make the music sound fuller, but the music itself is in the melody.  Unless, of course, there isn't any melody.  The harmony can call attention to itself by being interesting, but the point of this layer model is to show that the harmony doesn't <em>have</em> to be interesting to make a compelling piece of music.  I've recently been listening to Martinho da Vila's Casa de Bamba, which I think is a great song for reasons other than the harmony.  You can hear it in <a href="https://www.youtube.com/watch?v=VwsgCuQTlk0">this video</a>, which actually goes through three different hits by Martinho da Vila.  Casa de Bamba specifically begins at 2:36, but you should notice the songs before and after, which... are basically the same!  Same format, just slightly different melodies and different words.  Same beat, similar sound, and the same harmonies!  These songs are carried by the words (if you speak Portuguese, anyway) and the beat; the melody is repetitive and therefore it becomes familiar, which also carries the song.  There's more to it than that, too; these songs have call-and-response, which has a participatory component.  Where this falls in the layer model is left as an exercise to the reader.</p>
<p class="text">In contrast, Gyorgy Ligeti's <a href="https://www.youtube.com/watch?v=psMLOlCxsFw">The Alphabet</a>, has no melody to speak of, just harmony.  There's no beat; the words are the letters of the alphabet, so they're pretty much just mildly interesting placeholders.  Ligeti was a master of minimalism, able to make beautiful and compelling music lacking the upper layers we generally expect to encounter.  There's more than just harmony here; there's a little bit of melody as the voices move (since harmony is just a bunch of melodies at the same time), and there's a lot of play with dynamics, which we'll talk about in the lower layers.</p>
<p class="text">Jazz music also places a lot of emphasis on harmony.  Usually there's a melody at first, but then the music moves to improvisation over chords, and the melody becomes less distinct, and the main thing you get from the music is the harmony.  Here's John Coltrane's <a href="https://www.youtube.com/watch?v=30FTr6G53VU">Giant Steps</a>, one of the more famous pieces in the jazz repertoire specifically because of its harmony, the epitome of so-called <a href="https://en.wikipedia.org/wiki/Coltrane_changes">Coltrane changes</a>, referring to a chord progression that modulates by major thirds.</p>
<p class="text">Another time when you might hear music with the harmony layer on top is when the melody is just not playing for some reason.  A karaoke track might not have the melody, but it's actually fairly likely to have it in another instrument anyway because most people who sing karaoke need it (same reason that songbooks will almost always have the melody in the piano part).  If you're rehearsing with an ensemble and the people playing the melody just don't play, you get the same thing.  Conductors do this in order to focus on the background parts during rehearsal and fix any issues that might come up there, but you may find that it has other uses.  If you compose using a notation program like Sibelius, you can also select only the staves you want to play.  This gives you a new take on the music, since you've removed the top layer of melody.</p>
<p class="text">Mode is probably better thought of here as <em>macroharmony</em>, which is the harmonic context rather than just the current chord.  For example, if you play a C major chord in a C major context, that has a different sound from if you play a C major chord in a C lydian context.  The notes not in the chord are still in your mind, in melodies, as passing tones and neighbor tones, in previous chords, etc.  As we saw in <? sectionLinkShort('modes'); ?>, the mood of the piece is greatly affected by the mode, so much so that some people think that modes <em>have</em> moods.</p>
<h4 class="question">They don't?  Isn't major happy and minor sad?</h4>
<p class="text">Uh, no.  Not really.  Major <em>tends</em> to lend an air of happiness to a piece, but it could also just be melancholy or wistful or even painful, and these feelings depend on our cultural associations as well as the top layers — the beat, the tempo, the melody, the sound, etc.  Even the words have a big impact.  In <? exampleLink('Tristeza'); ?> we looked at a very happy-sounding song, based on mode, rhythmic feel, and melody, but the lyrics are sad.  That completely changes the character of the piece!  In addition, extra-musical conditions can also affect the mood, like when in your life a particular song was meaningful to you, context from a movie or concert, knowing about the history of the piece, stuff like that.</p>
<p class="text">And minor is <em>not</em> sad.  It's neutral at best.  You <em>can</em> write sad music in minor, but unless you really play it up, you don't really get negative emotions from minor modes.  I mean, is <a href="https://www.youtube.com/watch?v=N9qYF9DZPdw">White and Nerdy</a>, in G# minor, a sad song to you?</p>
<h4 class="question">No.  It's not.</h4>
<p class="text">No, it's not.  And the reason it's not is that minor is not sad.  It just isn't!</p>
<p class="text">Still, it <em>is</em> a downer <em>when compared directly to major</em>, because major is an upper.  So you'll find plenty of pieces that use this juxtaposition to achieve emotional goals.  For example, here's Simon and Garfunkel's <a href="https://www.youtube.com/watch?v=-BakWVXHSug">Scarborough Fair</a>, which we saw back in <? exampleLink('Scarborough Fair'); ?>.  The melody has just <em>one</em> instance of the dorian note, the 6, but the accompaniment plays up the difference between the minor i and the major IV in order to achieve the characteristic feel of dorian.  The musical layer we're speaking of is this characteristic feel.  It's created by the totality of the notes used over time, not just the ones used right now.</p>
<p class="text">The other layer in this section is rhythm.  Now, we've already talked about rhythmic feel — the beat and tempo, as well as other rhythmic effects like swing — but here I'm referring to the <em>actual</em> rhythm of the note values themselves.  If you think of rap, the beat is what the percussion (or drum machine) does; the tempo is how fast it does it; the rhythm is how the words the rapper is saying line up with the beat.  Here's Cardi B's <a href="https://www.youtube.com/watch?v=PEGccV-NOm8">Bodak Yellow</a>, which is a great example of varying rhythms in a meaningful way.  There are lots of eighth notes, and phrases tend to end with a dotted eighth sixteenth pattern, enough that the pattern is a distinctive motif for the song.  At a few points during the song, Cardi B goes into a triplet rhythm instead.  Throughout, the tempo and beat stay constant, but the variations (and constants) in the rhythm make the song memorable and unique.</p>
<p class="text">Some pieces employ more complex rhythmic ideas.  Way back in <? exampleLink('Mars rhythm'); ?> we saw the rhythm of Gustav Holst's <a href="https://www.youtube.com/watch?v=L0bcRCCg01I">Mars</a>, from The Planets.  If you listen, at 1:27 a new melody comes in with a contrasting rhythm.  Throughout the piece, there's a driving beat with the repeating 5/4 rhythm from <? exampleLink('Mars rhythm'); ?>, but when this new melody comes in, its rhythm doesn't line up with that rhythm.  This rhythm is just dotted quarter eighth, basically 2/4, on top of the 5/4.  This kind of juxtaposition is known as <em>polyrhythm</em>.  It's also <em>polyharmony</em>, since the repeated note in the 5/4 rhythm isn't even generally in the chords played or implied by the 2/4-like parts.  Holst was a big fan of doing multiple things at once!</p>
<p class="text">One important feature of rhythm is that it requires some sort of beat for context, even if it's a very weak one.  A rhythm is drawn in the canvas of the beat.  Music that has no beat has a kind of non-existent rhythm: things change over time, which is what rhythm is, but without a pulse of some sort, even if it's irregular, there's just no reference.  This is why the beat is at a higher layer than the rhythm; it's lack is much more noticeable.  To be clear, you don't need to be able to <em>hear</em> the pulse; you just need to be able to infer it from what you do hear.  If I sing a metrical melody by myself, there won't be a beat, but you'll easily be able to hear the rhythm, at least if I do it well enough!  You'll be able to clearly hear where the measures are, what the note values are, etc.  You'll even know when I put a rest on the downbeat (though it may take a few bars to figure it out).  That said, not everyone has a good sense of rhythm.  If you've been keeping up with the examples in this book, hopefully you've been developing yours.</p>
<p class="text">Oh, so!  When I was younger — much younger — I'd been in band for a few years, but my brother had only recently started playing trumpet with a teacher, before starting middle school band.  He was playing pieces with no context of accompaniment.  The result?  He'd rush the hell out of whole notes.  You're supposed to hold a whole note for the entire measure, and generally, if you're holding a whole note, other instruments are doing something more interesting during that time.  (There are times when the whole note is actually more interesting than the moving parts, like in a pedal point, but the point is that there <em>are</em> moving parts!)  So I'd be listening to him practice, and a whole note or a dotted half note or something would come up where other stuff happens in the music, and he'd just... cut it short!  Too boring for him!  Pissed me off, because I'd be thinking of the other parts and they wouldn't fit.  It took him a few years to develop a better sense of rhythm, but now he plays professionally so I assume he knows what he's doing.</p>
<p class="text">Anyway, the deeper layers are the most interesting ones:</p>

<h3 class="subsection-title" id="bottom-layers">8.4.3 The Bottom Layers</h3>

<p class="text">The bottom layers are what's left when you remove the top layers: dynamics, timbre, pitch, orchestration, and sound.  These are all properties of the sound itself: dynamics are the amplitude, timbre is the shape of the wave, and pitch is the frequency; orchestration is more about logistics — who does what — and sound is a combination of everything.</p>
<h4 class="question">Sound was in the top layer, though, wasn't it?  And isn't pitch just melody?</h4>
<p class="text">Yes and no.  To both questions.  THese bottom layers is where minimalism really lives, because the more recognizable aspects of the music are stripped away and you're left with, essentially, noise.  These bottom layers describe the noise.  We have dynamics, how loud or soft the noise is.  We have timbre, what the noise sounds like.  We have pitch, how low or high the noise is.  We have orchestration, which instruments are making the noise.  We have sound, basically whatever other aspects of the noise are left over.</p>
<h4 class="question">We're just talking about noise, then?</h4>
<p class="text">No, that was just by analogy.  In the physical sciences, <em>noise</em> refers to something fairly specific, which we'll talk about in a later chapter when we discuss the physics of sound.  The snares on a drum create a sound that is essentially noise.  On the other hand, the sounds in this bottom layer can be more conventionally musical sounds.  They could be notes on a regular piano, or syllables sung by a vocalist, or whatever.  Or they could be actual noises, like dropping a stool.  ...The kind you sit on.  Get your mind out of the toilet!</p>
<h4 class="question">Hehehe.</h4>
<p class="text">The first of these layers that we'll talk about is dynamics.  The basic crescendos and diminuendos that so primally tug at the heartstrings feel like they should be way up at the top layer.  Modern movie composers — not John Williams, but other, lesser ones — will compose incidental music with nothing but sweeping chords and crescendos and big climaxes, with none of the actual interesting stuff below.  Top layer bullshit!  So why is it down here?  Because dynamics are essentially form rather than content.  Thanks to the so-called "loudness wars", modern popular music doesn't even have dynamics; it's all just plain <em>loud</em>.  The tracks are actually run through a compressor to make sure that the dynamics are even.  Classical music, on the other hand, is built on dynamic contrast.  The dynamics can be really in your face, like with the big climaxes I was talking about, but they can also be far more subtle.  Here's the great Leonard Bernstein conducting Paul Dukas's <a href="https://www.youtube.com/watch?v=EMSXdKun1GU">Sorcerer's Apprentice</a> (which you hopefully know from Disney's Fantasia).  Pay close attention to the ebb and flow within sections, not just the sudden loud bits.  They convey musical meaning, even absent the other layers.  You could just have a snare drum roll get louder and softer, for example, and that would still contain the essence of dynamics.  Percussion ensemble music, which often can't rely on the other melody- and harmony-based layers, must depend on dynamics.  Here's John Alfieri's <a href="https://www.youtube.com/watch?v=a2L9GD-5JGY">Fanfare for Tambourines</a>, which demonstrates this concept as well as what you can generally do with, well, noise!</p>
<p class="text">Timbre gets modulated in different ways depending on the instrument or ensemble.  A choir, for example, can sing "ah" or "aw" or "oh" or "ooh" or many other vowels, or something in between these vowels.  You can try this yourself: sing a note on the brightest possible "ah" you can make, and over a few seconds gradually close it to the darkest possible "ooh" you can make.  It's pretty different, right?  You can do other things to your sound as well: introduce "eh" vowels, make your sound nasal (gradually go from "aw" to "ng", for example, and you'll hit nasal vowels about halfway), change your overtones...</p>
<h4 class="question">Change your overtones?  What?</h4>
<p class="text">Your mouth, tongue, and throat are a resonance chamber, and when you change the timbre, what you're actually doing, sound-wise, is changing the overtones that resonate in your mouth cavity.  There's a way you can preferentially make one overtone stand out, generally one of the ones between 6 and 12 inclusive, and it sounds like a kind of whistle.</p>
<h4 class="question">So I'd actually be singing two notes at once?  Bull.  Shit.</h4>
<p class="text">Listen carefully to the sounds made by <a href="https://www.youtube.com/watch?v=5tVGei24TdQ">Batzorig Vaanchig</a>.</p>
<h4 class="question">What the fuck is this?</h4>
<p class="text">Tuvan throat singing!  Tuva is in Mongolia, and it's famous for what we in the rest of the world would likely call extended vocal techniques.  It features sub-octave singing, which is when you use your vocal folds — a set of folds that aren't your vocal cords — to vibrate twice as slow as your true vocal cords and thereby make a sound an octave lower, and it also features overtone singing, where you make vowels with your mouth that produce a whistling overtone.  There's another technique that I can sometimes do that actually produces a note a twelfth below rather than an octave — or a twelfth above, I'm not sure.  I was singing something to the baby once and I really wanted to harmonize it with the note a fifth below, so... I did.  I'm not sure how that happened.  I can't reproduce it on command, but sometimes I do it anyway.  Producing more than one note at a time when logically you should only be producing one is called <em>multiphonics</em>.  Many instruments can actually do multiphonics, especially woodwinds, through clever combinations of embouchure (mouth position/shape) and fingerings, but multiphonics are generally limited to specific intervals on specific notes.  There are guides online to making multiphonics on various instruments if you're curious.</p>
<p class="text">Closely related to multiphonics is humming into your instrument.  You can hum and whistle at the same time, for example, and control each one quite independently.  It just takes a bit of practice.  Humming into the flute is very common.  Clarinet, less so, but one of the cool things about it is that the vibrations from the hum interfere with the vibrations from the clarinet, causing combination tones.  Some modern music uses this as an effect, as we can see in Carl Maria von Weber's <a href="https://www.youtube.com/watch?v=31BM9DlIbIU">Concertino in Em</a>, for horn and orchestra.  Oh, I'm sorry, did I say "modern"?  I meant <em>early Romantic</em>.  <a href="https://en.wikipedia.org/wiki/Concertino_for_Horn_and_Orchestra_(Weber)">He composed it in 1815</a>.  This performance by Javier Bonet is with a natural horn, too — look ma, no valves!  It's a horn in E (most are in F, or sometimes double horns in F and Bb that can be switched with a valve), because the concertino is in Em.  Throughout the concertino, you also hear the performer go between a smooth horn sound and a brassy (<em>cuivré</em>) sound, another kind of timbre modulation.  You can also see him changing the pitch by moving his hand in the bell, which sometimes inadvertently produces a cuivré sound.  Watching someone play fast runs on a natural horn — or try to, anyway — really makes you appreciate modern instrument construction, doesn't it?  About 9 minutes in, there's a big cadenza.  THe piece as a whole is a theme and variations, and the cadenza is another of the variations, but obviously with a lot of freedom for the performer.  Including <em>trills</em>.  With no valves.  This guy.  What was von Weber thinking?  The part right at 11:27 is where the humming comes in, with full chords created by the combination tones.  In 1815, so you know at least the voice leading is good!</p>
<p class="text">And then there's this act of wanton violence to the beloved tuba, Øystein Baadsvik's amazing <a href="https://www.youtube.com/watch?v=U0qIL2ie-VE">Fnugg</a>.</p>
<h4 class="question">Is he... Tuvan throat singing on a tuba?  WITH TUBA PERC?  What in the Seven Hells?</h4>
<p class="text">He even made a big show out of it with <a href="https://www.youtube.com/watch?v=mHMyrhilkdo">Fnugg Blue</a>!</p>
<h4 class="question">My world has been blown to bits!  Wide open!  I don't know what to believe anymore.  I don't remember, but I think maybe I used to be sane.  Was that sanity real or just a fleeting dream, a passing shadow?  No matter, it's gone now.  La la la.</h4>
<p class="text">As cool as this is, you <em>may</em> be overreacting a bit?  Singing into an instrument is pretty easy to do; it's just that most people don't think to do it!  And Baadsvik has managed to compose a really cool piece using it.  The real coolness of the piece actually lives in the top layers — the beat, the melody, the sound — and the novelty of the extended techniques on an unexpected instrument.  The timbre modulation when Baadsvik hums is interesting too, but if it weren't a novelty it wouldn't be particularly notable.</p>
<p class="text">Most instruments are actually capable of significant timbral variation.  On woodwinds, it's less, but you can generally change your embouchure to create fairly big sound differences.  On brass, embouchure matters, and different but similar instruments can have different sounds, like the trumpet, cornet, and flügelhorn, but the main way of varying timbre is by using mutes.  Trumpet players generally carry straight mutes (just pieces of wood or plastic that go into the bell), cup mutes (same but with a cup at the end, making a more "aw" sound), harmon mutes (they have a stick in the middle that you can move to change the timbre), and plunger mutes (literally a plunger head that you use your hand to move on and off the bell, popular in jazz).  Trombones generally only have to use straight mutes in orchestral music, but in jazz, all of the mutes come up.  Horns, euphoniums, and tubas generally only use straight mutes (a tuba mute is huge and funny-looking; look it up).  Brass instruments can generally make their sound cuivré.  Horn players have the additional ability to use their hand in the bell to stop the horn almost entirely, creating the stopped horn sound; this changes the pitch as well so you need to be aware of that when notating.  Percussion instruments have two ways of modulating timbre: adjust the instrument or adjust the beater.  A bass drum can be deadened with your knee; timpani can be muted a bit with felt pads; it's often a good idea to put a wallet or even just an ID card or credit card on a snare drum to make the vibrations neater.  You can use mallets of various types; on timpani, you have hard wooden mallets for a very hard sound, very soft cotton mallets for a very soft sound, spongy rubber mallets for in between, etc.  On marimba, you have different grades of yarn mallets, or hard plastic mallets of different levels of hardness; you do not want to use wooden or brass mallets because you will destroy your beautiful instrument.  On xylophone, wooden mallets are OK, but still, never use brass mallets on anything other than a glockenspiel!  You can also use the soft plastic mallets on the glock or vibes, etc.  There are various stick types for snare drums, including wire brushes, used commonly in jazz.  You can use wire brushes on a suspended cymbal too, or regular drumsticks or yarn mallets, or even a coin or a triangle beater scratched along it.  Even for the triangle, there are different-sized beaters you can use that will make a powerful sound or a tiny one.  And there are different sizes of triangle.  And different sizes of snare drum.  Different sizes of cymbals.  You can play a suspended cymbal on the edge, or in the middle, or on the bell.  Percussion music in general is a playground of timbre!  Oh, and everyone can participate: woodwind instruments can click their keys; brass players can press their valves or (carefully) tap their mouthpiece with their palm to send a burst of air through the horn.</p>
<p class="text">And don't think I forgot string instruments.  Oh no.  Strings have a lot of variation based on where on the instrument the sound is made.  Pluck or bow (there's another timbral variation) near the sound hole, and you get a nice, deep, resonant sound; play near the bridge and it gets tinny, which is useful for contrast.  Playing exactly halfway down the string creates a very interesting sound as well; on a guitar, this would be plucking the 12th fret of an open string.  It's almost bell-like.  With plucked strings, how the string is plucked — rest stroke versus free stroke, nail versus finger versus pick — makes a big difference.  With bowed strings, what part of the bow is used also makes a difference; this includes using the wood rather than the hairs on the bow, which is called <em>col legno</em>.  Speaking of bows, many percussion instruments can be bowed for some very interesting effects, including cymbals, crotales, glocks, vibes, and even tam-tams.  (Oh, and try putting a suspended cymbal on a timpani and changing the pedal, or, more weirdly, dipping a tam-tam in a big vat of water.  Or using maracas to beat a bass drum.  But we're not talking about percussion, right.)</p>
<p class="text">Electronic instruments are a whole other beast, and the possibilities there are virtually (ha ha) limitless.  If you have a Mac, play around with the effects in Garage Band.  If you don't, I'm sure there are free programs that let you take an audio track and apply standard effects to it, like reverb (adds echo), bitcrusher (makes it sound like a chiptune), flanger (applies a modulation to the timbre that you can look up yourself), chorus (adds detuned copies of the sound) etc.  You can use tunable EQ filters like highpass, lowpass, bandpass, etc.  You can combine digital effects with analog or acoustic signals, which is how you get the multitude of effects on a guitar pedal, as well as auto-tune for voice, and all the other cool stuff people do in popular music.</p>
<p class="text">Then there's the organ.  Organs are magnificent instruments.  A good pipe organ will be enormous, several stories tall, with a large assortment of pipes.  These pipes have different end behavior; each set of pipes sounds different based on how the air is forced out — some pipes have reeds; some have other mechanisms.  (A harmonica is essentially a teeny-tiny pipe organ!  Think about that!)  The cool think about the organ is that it has what are called <em>organ stops</em>, which are preset sets of pipes that may combine octaves, even add a pipe at the 12th, with different sound qualities, and thereby the organ can be made to have a huge sound, a tiny sound, a brass-like sound, a flute-like sound, a string-like sound, even a voice-like sound.  This variety of timbre is why modern synthesizers are sometimes called organs: the organ is a medieval additive synthesizer!  The modern version has very different means of sound production, being electronic rather than hydraulic, but it works on pretty much the same principles.  At least additive synths do.  There are other kinds of synths as well.  Synths tend to have a set of standard features like envelope control, low-frequency oscillators (for effects like vibrato or frequency modulation), multiple waveforms that can be independently tuned and rearranged in certain ways (to modulate the sound), probably a few biquad filters (...look them up; it would take too much math to explain what these are), EQ, and maybe a few other features.  If you're a synth expert, you spend a lot of time with your synth designing sounds using these features.  Some synths actually have analog circuitry for this stuff that gives it a warmer sound due to its being less mathematically precise.</p>
<h4 class="question">Isn't a synthesizer just an electronic piano?</h4>
<p class="text">I thought that for a long time too.  I have a synthesizer because ain't no way I'm going to put a real piano in an apartment (and also when I bought it I certainly didn't have enough money for a real piano).  Except calling it a synthesizer is misleading: it's an electronic keyboard.  It's one that happens to be fairly good at pretending to be a piano, and not much else.  Other keyboards have hundreds of sounds, rhythms you can use to accompany yourself, maybe even the possibility to record a snippet of your voice and play it back at different pitch levels (that's the instrument I learned on as a little kid, but that one was close to being a real synth).  Electronic keyboards are great.  If you don't have a piano, I highly recommend that you get an electronic keyboard; it doesn't have to be that good.  But it's not going to give you a lot of timbral variety.  A <em>synthesizer</em>, on the other hand, is all about customizing sound, not playing back samples.  If you're good at the synthesizer, that doesn't just mean that you can play Liszt on it or whatever; it means that you're good at designing the sounds you play with.  Synthesizers come in many sizes, too; since a synthesizer <em>isn't</em> a piano, there's no real requirement that they have 88 keys other than simple analogy.  In fact, they may even have more, allowing for some of the keys to act as controls for other sounds.  Some keyboards and digital synths also work as MIDI controllers, meaning that they can communicate with other devices using MIDI messages.  If you're lucky, they'll have a few wheels on the side that you can assign to certain MIDI message values.  These wheels are called <em>mod wheels</em>, and while the most common use is to use them to make pitch bends, you may be able to assign it to some aspect of the timbre and vary that on the fly.</p>
<p class="text">There's a lot more you can do with timbre; I'm sure you can come up with your own ideas.  Timbre is pretty much magic.  So why is it down in the lowest layer?  Because it doesn't actually carry much musical <em>meaning</em>.  Other than its obvious impact on the overall sound of the music, timbre is not something listeners generally pay attention to unless they're interested in how the music is made (which I assume is why you're reading this book, but anyway).  It usually takes a lack of some of the higher layers to make the timbre action salient enough.</p>
<p class="text">The next layer is pitch.</p>
<h4 class="question">But melody is pitch.  Harmony is pitch too.  It's all pitch.  That's what we're studying in this book, no?  Why is pitch hidden below melody if melody is already pitch?</h4>
<p class="text">Good question.  To answer that, it's probably simplest to play an example.  Here's <a href="https://www.youtube.com/watch?v=ffw6tjvJHw0">Rock Island</a> (starts about 1:49), from Meredith Wilson's The Music Man.  While you're listening to this, pay attention to the pitch inflections.  There are three main things going on in this piece, which consists of some number of men talking, not singing.  There are the words, in the top layer; there are the rhythms, in the middle layer; there is the pitch, in the bottom layer.  You generally wouldn't consider the inflections of speech to be a melody, yet they are extremely important to the character of Rock Island.  By the way, this is from the Broadway version; the <a href="https://www.youtube.com/watch?v=JZ9U4Cbb4wg">movie version</a> is somewhat different but still worth watching, because they integrate the train's movements into the scene as well.  Of course, I could have shown you rap instead of Broadway, but I feel like I've shown you plenty of that already.  Rap music also places this kind of emphasis on inflection.  I've shown it to you earlier in the section, but Cardi B's <a href="https://www.youtube.com/watch?v=PEGccV-NOm8">Bodak Yellow</a> has motifs based on inflection as well as rhythm; the phrase "bloody moves", for example, is not only a dotted eighth sixteenth quarter pattern, it also starts high in pitch and ends low; both of these features recur at phrase endings throughout the song.  Cardi B's treatment of rhythm in this song is especially noteworthy, but the way she handles pitch is pretty much the same as any other competent rapper, since modulating the pitch of the rapping is one of the most fundamental aspects of rap music.</p>
<p class="text">Pitch is meaningful when speaking in analogy to how melody is meaningful when singing.  I'd argue that the difference between speaking and singing (other than any sort of proper technique you might want to use when singing) is that speaking isn't actually hitting <em>notes</em>, just pitches, while singing is indeed hitting notes.  But the difference is actually psychological rather than physical; if you're clever, you can <a href="https://www.youtube.com/watch?v=9nlwwFZdXck">figure out</a> which notes are being spoken.  Generally, speech doesn't happen on the same plane as singing.  They don't harmonize with each other; they don't clash with each other.</p>
<p class="text">The same goes for unpitched percussion.  Some unpitched percussion is actually lying about being unpitched.  Tom-toms are definitely pitched, for example.  You can figure out what pitch they are (and you can tune them too, usually, by tightening or loosening the drum head with a drum key); the exact pitch just (usually) doesn't matter.  What matters is that you have an array of several toms at different pitches, some higher and some lower, so that you can play all of them and make effects.  Quads in a marching band work on the same principle; there are four of them, but the exact pitch doesn't really matter.  Marching bands often have several bass drums in different pitches as well; again, tuning doesn't really matter, but they're still different from each other.  By the way, I say that tuning doesn't matter, but actually it does, just only at a very high level of performance quality.  Most people would never even notice the difference.</p>
<p class="text">Other unpitched percussion may really be unpitched, but it can still be higher or lower.  A suspended cymbal is definitely unpitched no matter what, but the smaller the cymbal, the higher it sounds, from a big low 24-inch monstrosity to an 8-inch splash cymbal.  Snare drums can be bigger or smaller, which means lower or higher pitch, respectively; Persichetti's <a href="https://www.youtube.com/watch?v=xjocQ4SQuVQ">Symphony No. 6 for Band</a> happens to use three different snare drums.  With the snares off in the first movement, they just sound like three toms, but with the snares on in the fourth, it's a different kind of sound.</p>
<p class="text">Even pitched instruments can have non-melodic pitch.  What matters here is context.  If the music is highly chromatic, you may get a general sense of "instrument playing low" or "instrument playing high" rather than some sort of melody.  Pitch is also more absolute than melody; you could play the same melody at a higher pitch level and it's still the same melody, but when we're talking about pitch, we care about absolute pitch, not relative pitch.  You could have some mass of sound that happens to be in lower registers or middle registers or whatever.  Since I first realized this with Ligeti's <a href="https://www.youtube.com/watch?v=FAuAFEFEFSo">Ramifications</a>, let's listen to that piece, played by 12 strings.  There's nothing approaching a melody, but things do get higher and lower.  There's texture (part of the sound of the piece), but there's nothing you'd normally call melody or harmony, only pitch.  The interest in this piece comes from listening to these slowly-evolving masses of texture go up and down, hitting interesting sounds along the way.</p>
<p class="text">Notes generally stop having melodic meaning when they aren't in the context of a scale or some fixed point.  You can see this with atonal music, though you do get intervals, which establish some sort of pitch center, if only briefly.  Atonal composers often deliberately avoid intervals that may accidentally imply tonal centers in order to keep their music as melody-free as possible to the ear.  That isn't to say that melodies <em>aren't there</em>; the idea is to convince the ear not to treat them as such and treat them as simple low-level pitch variations instead.  Ways to do that include using small intervals as much as possible and avoiding major thirds and perfect fifths as much as possible.  Minor thirds are less problematic.  An even more destabilizing idea is to use microtonality, which is what Ligeti does in Ramifications by having half the ensemble tuned a quarter step sharp.  Still, it's quite easy to have atonal melodies if you create textures where a single voice comes out coherently.  Ligeti avoids this by having long notes, tremolos and similar figures, single-note interjections, stuff like that.</p>
<p class="text">Even with melodic meaning, pitch is relevant.  Notes can feel high or low, for example.  If you effect a modulation, you create a new tonal center, and the entire piece will feel higher or lower depending on how you do it.  Hearing a single tonal center can get stale if you do it too long, which is why people have modulations.  In popular music, which usually has a verse/chorus form, the chorus may be at a different pitch level than the verse, and if there's a bridge, that's usually also pretty different.  Even switching from authentic to plagal (or vice-versa) will lessen the tonal fatigue.  We can also generally feel the effort that it takes to go very high on an instrument (or very low, if applicable), and this aspect of pitch adds another dimension to our enjoyment of the music.  The same melody played an octave up on the trumpet may wow audiences even more.  With voice it's even more dramatic, since we all have voices and generally know how it feels to sing very high or very low.  We may even feel it sympathetically when <em>other</em> people sing, and just singing high is an achievement.</p>
<p class="text">Pitch, apart from melody, can get overlooked as a compositional element, but it's actually responsible for a lot of what we may hear but not immediately understand about a piece of music.  I mentioned several applications, but I've barely scratched the surface.  One interesting aspect of pitch is that it obeys the Uncertainty Principle of quantum mechanics.</p>
<h4 class="question">Oh no.  It can half-kill cats?</h4>
<p class="text">Dude, that's not the Uncertainty Principle.  The Uncertainty Principle says that the better you know a particle's location, the worse you know its velocity, and vice-versa.</p>
<h4 class="question">To within <del>h</del>/2, right?  But how does this relate to pitch?  <del>h</del> is very, very, very small.</h4>
<p class="text">Well, because that's not really what the Uncertainty Principle is all about.  See, what it actually means is that if you have some function and its Fourier transform, the more spiky one of them is, the more spread out the other one is.  We'll talk about Fourier transforms later when we discuss sound waves; the relevant bit is that the Fourier transform of a sound is essentially a graph of the frequencies in that sound wave.  So if you have a very long sound, you can have a very clear, spiky frequency, while if you have a very short sound, the frequency is not so clear.  There's a compounding issue here, which is that pitch is actually logarithmic in frequency: <em>add</em> to the pitch one octave and you <em>multiply</em> the frequency by 2.  You may then have a pitch uncertainty of, say, ±1 Hz.  C1 is about 32.7 Hz; B0 is 30.9 Hz; C#1 is 34.6 Hz.  ±1 Hz is an error of about half a semitone!  The same error around C6 will be 32 times smaller in pitch, so only 1 or 2 cents.  We'll discuss this in more detail later.</p>
<h4 class="question">With pictures?</h4>
<p class="text">With pictures.  It will make sense and you won't have to do any math yourself.  Don't worry!</p>
<p class="text">The Uncertainty Principle only makes a difference for very short sounds, like a string bass slap.  But like a string bass slap, it's not the whole story.  When a sound starts, there's an articulation of some sort.  That articulation is going to cause what are called <em>transients</em>, effects that go away over the course of the sound.  If the sound ends too soon, though, the transient effects may not have had time to go away yet, so the pitch may still be uncertain.  Again, we'll explore sound waves in more detail eventually, but the point for now is that short, low sounds can have rather unspecific pitch.  The upshot?  You can make a string section sound like percussion by playing a chord of some sort very staccato!  And you can vary the sound of that string hit by changing the notes of the chord — the chord is too short to hear anyway!  The other side is that if you have short chords and you <em>do</em> want to hear them, you have to play them a bit longer.</p>
<p class="text">Compared to pitch, there's less to discuss about orchestration as a musical layer, which is the distribution of parts within the ensemble and, generally, the choice of instruments.  Orchestration itself is actually a huge topic worthy of its own chapter, but for the purposes of this discussion, it simply refers to the choices of sounds.  Sounds can have spatial position, too, which is occasionally used as a compositional element.  For example, a piece may use two choirs, which generally would be placed on opposing sides of the stage.  There could be antiphonal trumpets, placed on a balcony,  There could be musicians placed throughout the hall or even in the audience for a very surround-sound experience.  There could be an offstage ensemble.  These effects are mostly difficult to show on video, but you can get an approximation: Frank Ticheli's <a href="https://www.youtube.com/watch?v=K4MWarA7V7U">An American Elegy</a>, written for the victims and survivors of the 1999 Columbine massacre, uses this effect around 8:12.  A trumpet player plays a solo from offstage to symbolize the victims, some of whom were in the wind orchestra at Columbine HS.  Those victims' seats were filled with flowers, but they were still playing in spirit during this trumpet solo.  I was fortunate to play this piece in 2000 under the direction of Prof. Allen McMurray from University of Colorado — Boulder, a director who had worked with the wind orchestra at Columbine and probably knew some of the victims, since he broke into tears after we performed it.  The offstage trumpet symbolizes the victims watching from Heaven, given the trumpet's association with angels.  Unfortunately, you have to imagine the magic, because instead of an angelic trumpet from Heaven, all you hear in the video is bad mixing.  Spatial stuff can sometimes work with stereo or surround sound, but it only really works if you're actually there.</p>
<p class="text">When you write a piece of music for some ensemble, you always have to decide who plays what.  Often, this decision is stupidly obvious.  If you're in a standard four-person rock band, the bass notes go to the bass, the chords go to the guitar, the percussion goes to the drums, and the lyrics go to the vocals.  Obviously.  But things can always get more interesting.  The guitar can do complete chords on its own, which could leave the bass free to solo.  The other players could do back-up vocals along with the lead singer.  The lead singer can clap and help out the drummer, or hey, why not sing while <a href="https://www.nbc.com/saturday-night-live/video/more-cowbell-with-will-ferrell-on-snl--video--saturday-night-live--nbc/n41046">playing cowbell</a>?  With a bigger group, it gets more complicated.  You have a high melody; should it go to the flutes?  The oboes?  Clarinets?  Trumpets?  Soprano sax solo?  Violins?  Some combination of these?  As the orchestrator, you have to decide which timbres to go for, and each instrument is agile in different ways in different places, with different articulations.  A violin would play a melody very differently from a glockenspiel, even on the same notes.</p>
<p class="text">Different instruments can also have very different affective qualities.  I mentioned the angelic trumpet from An American Elegy, which is a trumpet because angels are associated with trumpets — and the trumpet also features in military remembrances of the dead in <a href="https://www.youtube.com/watch?v=Bfe4TxvUOiw">Taps</a>.  Other instruments carry other connotations.  A brass choir can sound like a pipe organ, leading it to sometimes sound church-like (and churches do sometimes employ brass ensembles at important services).  Percussion can be martial; chimes (also known as tubular bells) can sound like church bells at a funeral; flutes can sound playful and woodsy, saxophones immediately call to mind jazz, even in an orchestra; oboes and especially English horns can be pastoral; horns can also be pastoral or signal hunting; etc.  When you're around these instruments enough, you readily ignore these associations because you know that the instruments are capable of much, much more, but composers do sometimes make references to these.</p>
<p class="text">I consider orchestration separately from timbre in that a change in instrument is categorical, while a change in timbre is a question of degree.  You may disagree with this separation, but what really matters is the idea that interesting happenings in the orchestration of a piece, like with the timbres used in the piece, are not as salient as the higher layers.  The orchestration is less important to the character of a piece than, say, harmony or melody.  Indeed, it's common to write transcriptions of pieces originally scored for one type of ensemble into other types of ensembles.  We've seen several transcriptions so far in the book, so I don't need to rehash.  As a wind orchestra musician in school, I ended up playing many wind orchestra transcriptions.  We played, for example, a wind ensemble version of the finale of Shostakovich's <a href="https://www.youtube.com/watch?v=_wdQmkp6vOE">Symphony Nº5</a>, transcribed from the <a href="https://www.youtube.com/watch?v=T1h1NJMKtnc">symphonic orchestra</a>.  They're the same piece, just played by different instruments.  The difference is just in the orchestration.  We also played a wind ensemble version of Shostakovich's <a href="https://www.youtube.com/watch?v=bFRoAbDo85g">24 Preludes, Op.34, Nº14</a>, transcribed from the <a hre="https://www.youtube.com/watch?v=Za0itRJRq7Q">piano</a>.  These are a lot more different, thanks to the overall sound (top layer) being very different.  The transcriber also took the liberty of adding things not possible in the original medium, like crescendos and percussion, which is a nice possibility when doing a transcription, but it's a tricky one to get right.  The cymbal crash at the climax makes sense, but one could imagine a Paul Muriat-style transcription with a drum set beat.  Oh, HELL naw!</p>
<h4 class="question">Can you <em>do</em> that?  Can you just shit on Shostakovich's grave like that?</h4>
<p class="text">I like to think that the universe has a way to prevent that kind of thing, but I'm not so sure...</p>
<p class="text">The last layer I'm going to talk about in this group is sound.  We've already discussed how the overall sound of a piece is way upper-layer stuff, but the details are lower-layer.  These details are mostly about the <em>texture</em>, which is kind of hard to define, but I'm going to try to explain it anyway: the texture is how the different parts are organized.  At the most basic level, we can talk about a <em>monophonic</em> texture — every voice has the exact same thing (possibly in octaves), which by default includes music for just one voice — a <em>heterophonic</em> texture — every voice has pretty much the exact same thing, but with small embellishments or variations — a <em>polyphonic</em> texture — different voices have their own roughly independent melodies — and a <em>homophonic</em> texture — the voices have similar rhythms but different notes in order to form chords.  These aren't very descriptive, though, and it's easy to think that texture refers just to these possibilities.  An interesting texture could be, for example, a beat pattern in the horns and bassoons in close harmony supported by percussion with the melody played by the bassoon and piccolo three octaves apart.</p>
<h4 class="question">What might that sound like?</h4>
<p class="text">It might sound like the beginning of Mikhail Ippolitov-Ivanov's <a href="https://www.youtube.com/watch?v=MueWVRMFodk">Procession of the Sardar</a> from his Caucasian Sketches.  Caucasian as in from the Caucasus region, where Turkey is (and several other countries), not as in white people.  If you listen to the piece, the texture continues to change as more instruments are added, as polyphonic countermelodies come in, chords are revoiced, etc.  The percussion stays almost constant, though, providing something of a lodestar for the listener while other things change.  Some composers (looking at <a href="https://www.youtube.com/watch?v=X0TPeuSm23Q">you, Vincent Persichetti</a>) like to change textures often, and it can get a little too same-y, ironically, to completely change everything every eight bars.</p>
<p class="text">An interesting texture can make a piece of music — well, a musical moment, at least — stand out.  But you have to be listening for it.  That's the difference between the top layer of sound and this detail layer; the top layer kind of blows you away or doesn't, but the bottom layer is the appreciation for the moments of the music.  And if you have music with a very weak top layer and a very weak middle layer, the texture becomes really interesting.  Listen to the textural changes in Steve Reich's nondescriptly-titled <a href="https://www.youtube.com/watch?v=ZXJWO2FQ16c">Music for 18 Musicians</a>.  Everything changes slowly, so you get the chance to appreciate each bit of texture and sound.  There is harmony and macroharmony, but the changes are too slow for them to dominate the lower layers.</p>
<h4 class="question">So I listened to the first 5 minutes and nothing happened.  Should I listen through to the end?  It's over an hour long!</h4>
<p class="text">Uh, no, it never gets exciting.  But if you look at the guys playing marimba on the left, they're doing the exact same thing for, like, half an hour, and then they both take a bathroom break or something when the music changes.  Probably the most exciting thing in the piece is those two guys not playing anymore.  You've got to admire the fortitude of these musicians.  They probably <em>rehearsed</em> this piece, too.  I don't know what kind of spiritual or medical state you have to be in to play this whole thing without your arm falling off.  I mean this even for the singers, who use their arms to slowly move the microphone from one side to the other.</p>
<h4 class="question">Wow, uh, who <em>listens</em> to this stuff?</h4>
<p class="text">...Yeah, I don't know.  <a href="https://en.wikipedia.org/wiki/Music_for_18_Musicians">David Bowie</a>?  He calls it "Balinese gamelan music cross-dressing as minimalism", which isn't too far off, except for the fact that a gamelan is not tuned to 12-TET so there are other kinds of features in the sound.  I'll tell you, though.  Listening to Music for 18 Musicians <em>really</em> makes you appreciate silence.  It is hypnotic, though, which is part of what Reich was going for.  In that sense, it's similar to Terry Riley's <a href="https://www.youtube.com/watch?v=yNi0bukYRnA">In C</a>, which can be similarly long and features hypnotic continuous 8th note C's, with the crucial difference that Reich's piece is meticulously composed, while Riley's piece is in part left up to the individual performers.  Reich's piece allows for specific changes in the sound, while Riley's piece focuses on other factors instead.</p>
<h4 class="question">What other factors?</h4>
<p class="text">That's an interesting question.  The model is somewhat limited because we're only talking about music, but we can always take a more holistic approach:</p>

<h3 class="subsection-title" id="beyond-layers">8.4.4 Beyond the Layer Model</h3>

<p class="text">In addition to the layer model, there are extra-musical and compositional factors that influence our perception and enjoyment of music.  The interesting thing about them is that you have to actually have knowledge about them to appreciate them.  Terry Riley's <a href="https://www.youtube.com/watch?v=yNi0bukYRnA">In C</a>, mentioned above, is an example: the piece consists of 53 chunks of music that each get repeated an arbitrary number of times by each performer.  If you know this, you'll be paying attention to when individual voices change and how that impacts the sound of the piece.  If you <em>don't</em> know this, you're listening to something repetitive and annoying that you wish would just end.  I mean, maybe you're thinking the same thing even if you already know how the piece is built, but that knowledge goes a long way to appreciating the music.  Music with random elements is called <em>aleatoric</em>, and appreciating it pretty much requires the understanding that it <em>is</em> aleatoric.  Probably the most famous aleatoric and minimalist piece is John Cage's 4'33", which consists of 4 minutes and 33 seconds of audience noise.  We've talked about it quite a bit so far.  Where does this fit in the layer model?  There's no beat, no melody, no words, <em>no sound</em>, no harmony, no macroharmony, no rhythm, no dynamics, no timbre, no orchestration, no texture, <em>no sound</em>.  You can't make sense of 4'33" using these terms.  As a physics/math person, this makes perfect sense to me: all the terms are 0 and that means that the music itself is 0, which it is because it's literally just silence.  But that's not quite right.  First, what do you get when you remove all sound from the performance?  The sound of the audience.  That was the point of the piece.  There's a layer down beneath all the others: the audience.  But here too, unless we already know to listen to the audience, we'll just be bored for two hours.</p>
<h4 class="question">Isn't the piece just four minutes and thirty-three seconds long?</h4>
<p class="text">If the concert you're going to features 4'33" and you don't understand how to appreciate it, chances are the rest of the concert won't be that interesting to you either, right?</p>
<p class="text">On a completely different note, Bach's canons often tend to be much better appreciated when you know what's going on.  If you just listen to the <a href="https://www.youtube.com/watch?v=xUHQ2ybTejU">Crab Canon</a> (one instance starts at 2:04),it's just pretty polyphonic music.  Try listening to the entire Musical Offering in the background or something.  Pretty.  But.  If you know how the Crab Canon is organized, it's much more interesting!  There's just one line of music, but it's being played forwards and backwards at the same time.  That's cool, right?  Another one is from the Art of Fugue, the <a href="https://www.youtube.com/watch?v=n0j5cTIwFV8">Canon in Augmentation and inversion</a>, where the second voice copies the first, except upside-down and half as fast, so ascending eighth notes in the first voice becomes descending quarter notes in the second.  You're unlikely to hear this relationship without already knowing that it's there (or looking for it), but if you do know about it, it provides another layer of musical meaning to the piece.</p>
<p class="text">These aspects of the composition are at least <em>musical</em>.  There's also the extra-musical side.  We looked at some Shostakovich pieces earlier; it might be useful to know that he was a Soviet who hated Stalin and lived in constant fear of being disappeared by the government at any moment.  The constant fear was justified; Shostakovich's music was too abstract for Stalin's artistic program, and he would occasionally not be allowed to have some of his works performed because they did not properly glorify Soviet communism.  In particular, the Finale of his Symphony Nº5, discussed earlier in this section, has a few bits that we now know are references to Stalin himself being laughed at.  Of course, if the censors had found out, there'd have been no Symphony Nº6, much less Symphony Nº15.  Dude was prolific.  When you listen to the moment in question, your knowledge of the extra-musical feelings of the composer affect your understanding and appreciation.  On a much nicer note, you could say that his American counterpart was Aaron Copland, staunch left-winger, believer in freedom and America and the common man (he even wrote a <a href="https://www.youtube.com/watch?v=FLMVB0B1_Ts">Fanfare for the Common Man</a>).  But in the 1950's, with McCarthyism, he was hounded for his leftism, and he saw the promise of the patriotic America of WWII go away.  If you know what his music represents, you can understand his tonal shift — tonal in the sense of tone, not tonal music — with the changing political landscape.</p>
<p class="text">There's also, of course, <em>program music</em>.  That's music that's meant to tell a story.  The music kind of doesn't make so much sense without the story!  Berlioz's <a href="https://www.youtube.com/watch?v=0DWjI1uLSzw">Symphonie Fantastique</a> is an example; you can read <a href="https://en.wikipedia.org/wiki/Symphonie_fantastique">the program</a> as well.  The program explains the images the music is trying to conjure (there's a particular moment in the fourth movement that involves a guillotined head bouncing on the ground).  I'm not going to make excuses for the plot.  It's a <em>terrible</em> plot.  Luckily, the plot itself isn't that important; all it does is provide context.  But that context is important!</p>
<p class="text">Our last element is familiarity.  We tend to underestimate familiarity, in part because it creeps up on us slowly, but familiarity is as powerful as the top layer.  The better you know a piece of music, the more you'll enjoy it.  It's a huge part of why we like the music we like.  We start out not liking anything in particular, but through our socialization as children and teenagers, we end up listening to some genre or style more than others.  We listen to the radio, which plays the same songs over and over; we get frequent exposure.  We can't help but like it and consider it good music, unless it triggers hatred instead.  Individual pieces of music work on this principle as well, introducing some theme or motif, exploring it, and returning to it triumphantly, which relies on our growing familiarity with the theme to make us feel stirred by the return.  Fugues are built on this concept, as is anything in sonata form, like the first movements of most sonatas and symphonies.  It's also why videogame music is so catchy: here, you're playing a game and listening to its music on loop, so of course the music is going to be special to you.  It's how leitmotif works in long-form narrative works like opera and film: a theme is associated with a story element, so when we hear the music, we automatically get some of the basics of the story given the presence of leitmotifs.  John Williams is a master at this; see if you can find for purchase the Star Wars Special Edition soundtracks (from the original trilogy), which have extensive liner notes detailing the motifs heard and what they represent in the story at that point in the film.  Just above I mentioned Berlioz's <a href="https://www.youtube.com/watch?v=0DWjI1uLSzw">Symphonie Fantastique</a>; that symphony has what's called an <em>idée fixe</em>, a single melodic idea that pops up repeatedly throughout the symphony.  The idée fixe represents the protagonist (which <a href="https://en.wikipedia.org/wiki/Symphonie_fantastique">the program</a> tells you), so you can hear for yourself what happens at the end of the fourth movement.  I won't spoil it for you!  The fifth movement also employs familiarity by referencing <a href="https://en.wikipedia.org/wiki/Dies_irae">Dies Irae</a>, a well-known plainchant hymn associated with the funeral service in Catholicism.  If you know Dies Irae, which Berlioz kind of expected you to, when it pops up in the music it will tickle your fancy as being familiar <em>and</em> call to mind the deathly associations you would normally have with that melody.</p>
<p class="text">Finally, in addition to these layers, you may well have some layers of your own, from your own understanding of music.  I make no claim that my layer model is <em>right</em>.  It's just my model.  You may well prefer a different analogy altogether.  That's OK.  But it's good to understand that there <em>is</em> a hierarchy when it comes to what's important in a piece of music, that some elements are generally more salient to a piece of music's identity and appreciation than others.  You may disagree with the order exactly, which is also OK.  I think that my model here makes sense: at the top level, we have the stuff the brain is hard-wired to hear; at the middle level, we have the stuff we've culturally built up about music; at the bottom level, we have the properties of sound itself; here outside the layers, we have the meta-musical features from learning and thinking <em>about</em> music rather than the music itself.  I hope you think my model makes sense too.</p>
<p class="text">Next we'll talk about one of the most overlooked aspects of composition, because <em>someone</em> has to, right?</p>

<?php

createPageFooter($label);

?>